# -*- coding: utf-8 -*-
"""A4_2019369_QA_autograd

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOkWq_PBl4iI29vDHpDIKQQnqQ5CWwSI
"""

import torch
import numpy as np
import math
import torch.nn as nn
import matplotlib.pyplot as plt



n=1000
mean=[0,0,0]  #mean-given
cov=[[1,0.8,0.8],[0.8,1,0.8],[0.8,0.8,1]]
data = np.random.multivariate_normal(mean, cov, n)
data_tensor = torch.tensor(data)
print(data_tensor)
print(type(data_tensor))
#dividing testing and training data
train_sample=data_tensor[0:800]
test_sample=data_tensor[800:]
print(type(train_sample))
print(type(test_sample))
max_elements, max_idxs = torch.max(train_sample, dim=0)
print(max_elements)
min_elements,min_idxs=torch.min(test_sample,dim=0)
print(min_elements)
max_elements_test, max_idxs_test = torch.max(test_sample, dim=0)
print(max_elements_test)
min_elements_test,min_idxs_test=torch.min(test_sample,dim=0)
print(min_elements_test)
train_sample_normalised=(train_sample-min_elements)/(max_elements-min_elements)
test_sample_normalised=(test_sample-min_elements_test)/(max_elements_test-min_elements_test)
W1=torch.tensor([[1 ,1, 1],[1,1 ,1]],dtype=torch.float32,requires_grad=True)
W2=torch.randn((3,2),dtype=torch.float32,requires_grad=True)
bias1=torch.tensor([[0],[0]],dtype=torch.float32,requires_grad=True)
bias2=torch.tensor([[0],[0],[0]],dtype=torch.float32,requires_grad=True)

print("bias1. "+str(bias1))
print("bias2. "+str(bias2))

def sig(val):
    res=1/(1+torch.exp(-val))
    return res

# model output
def forward(X):
    X.resize_(3,1)
    input1=torch.matmul(W1,X)+bias1
    #print("input1. "+str(input1))
    input1=input1.float()
    output1 = sig(input1)
    output1=output1.float()
    input2=torch.matmul(W2,output1)+bias2
    #print("input2"+str(input2))

    output2 =sig(input2)
    return output2



# Training
W1=torch.tensor([[1 ,1, 1],[1,1 ,1]],dtype=torch.float32,requires_grad=True)
W2=torch.tensor([[1,1],[1,1],[1,1]],dtype=torch.float32,requires_grad=True)
bias1=torch.tensor([[0],[0]],dtype=torch.float32,requires_grad=True)
bias2=torch.tensor([[0],[0],[0]],dtype=torch.float32,requires_grad=True)

#print(bias2)
learning_rate = 0.01
n_iters = 150
training_loss_per_epoch=[]
test_loss_per_epoch=[]
epoch_list=[]
for epoch in range(n_iters):
    # predict = forward pass
    sum=0
    sum1=0
    for X in train_sample_normalised:
      X=X.float()
      y_pred = forward(X.float())
      bias1=bias1.float()
      bias2=bias2.float()
      #print("y_pred")
      #print(y_pred)
      #print("initial input")
      X.resize_(3,1)
      #print(X)
      # loss
      mse_loss = nn.MSELoss()
      l = mse_loss(X,y_pred)
      
      #print("loss"+l)
      l=l.float()
      l.backward()

      # update weights
      #w.data = w.data - learning_rate * w.grad
      #update weights, this operation should not be part of the computational graph
      with torch.no_grad():
          W2 -= learning_rate * W2.grad
          W1 -= learning_rate * W1.grad
          bias1-= learning_rate * bias1.grad
          bias2-= learning_rate * bias2.grad
      
      W2.grad.zero_()
      bias1.grad.zero_()
      bias2.grad.zero_()
      W1.grad.zero_()
      sum=sum+l.item()
    training_loss_per_epoch.append(sum/800)
    epoch_list.append(epoch)
    for X in test_sample_normalised:
      X=X.float()
      y_pred = forward(X.float())
      bias1=bias1.float()
      bias2=bias2.float()
      X.resize_(3,1)
      mse_loss = nn.MSELoss()
      l1 = mse_loss(X,y_pred)
      sum1=sum1+l1.item()
    test_loss_per_epoch.append(sum1/200)

#print(epoch_list)
#print(test_loss_per_epoch)

plt.plot(epoch_list,training_loss_per_epoch,label='train') #blue -- training
plt.plot(epoch_list,test_loss_per_epoch,label='test',color='black') #green -- test
plt.xlabel('epochs')
plt.ylabel('error')
plt.legend()
plt.show()
